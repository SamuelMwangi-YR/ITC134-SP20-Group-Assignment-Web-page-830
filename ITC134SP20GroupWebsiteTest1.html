<!DOCTYPE html>
<html>
<head>
<title>ITC 134 Group Assignment WebPage</title>
<title>Group Topic: Software Releases</title>
<style>
div {
margin: 0px auto;
width: 50%;
}
p {
margin: 0px;
}
.center {
margin-top:10%;
height:50px;
text-align: center;
}
header, nav, footer {
background-color: rgb(13, 226, 241);
text-align: center;
width: 850px;
}
main {
margin: auto;
height: 2400px;
background-color:  rgb(126, 127, 128);
overflow:hidden;
width: 850px;
}
nav, footer {
background-color:  rgb(148, 148, 148);
color: white;
width: 850px;
}
section.left {
text-align: center;
height: 2400px;
background-color:  rgb(7, 222, 250);
float: left;
width: 200px;
}
ul {
margin-left: -40px;
list-style-type: none;
}

</style>
</head>
<body>
<div>
<!-- Header Start -->
<header>
<!-- Banner Start -->
<h1>ITC 134 Group Assignment WebPage</h1>
<h1>Group Topic: Software Releases</h1>

<!-- Banner End -->
<!-- Nav Bar Start -->
<nav>
<a href="#" title="Class Assignments">Home</a> &nbsp;|&nbsp;
<a href="#" title="Class Assignments">Class Exercises</a> &nbsp;|&nbsp;
<a href="#" title="Home Assignments">Assignments</a> &nbsp;|&nbsp;
<a href="#" title="Resourses">Resources</a>
</nav>
<!-- Nav Start -->
</header>
<!-- Header End -->
<main>
<!--Link Section Start -->
<section class="left"><br>
    Mwangi Samuel:
<p>Links</p>
<hr>
<ul>
<li>
<a href="https://www.nap.edu/read/4761/chapter/12" title="Computer Hardware and Software for the Generation of Virtual Environments">Refrence#1</a>
</li>
<li>
<a href="https://phys.org/news/2017-12-future-hardware-ai.html" title="The future of hardware is AI">Refrence#2</a>
</li>
<li>
<a href="https://docs.google.com/document/d/169MGSTx8UDK0YJni6q-eVt5U4GYS26RJTFIIhqE8NiM/edit?ts=5ebd7371#" title="Google Docs">Group Google Docs</a>
</li>
</ul>
</section>
<!-- link section Start -->

<!-- Side Bar Start -->
<section class="center">
<p>Future of Software Releases</p>
</p>By ; Mwangi Samuel Njoroge</p>
<br>
<br>
    </p>The future of hardware is AI</p>
<br>
    AI workloads are different from the calculations most of our current computers are built to perform. AI implies prediction, inference, and intuition. But the most creative machine learning algorithms are hamstrung by machines that can't harness their power. Hence, if we're to make great strides in AI, our hardware must change, too. Starting with GPUs, and then evolving to analog devices, and then fault-tolerant quantum computers. First, we'll utilize GPUs and build new accelerators with conventional CMOS in the near term to continue; second, we'll look for ways to exploit low precision and analog devices to further lower power and improve performance; and then as we enter the quantum computing era, it will potentially offer entirely new approaches. Accelerators on CMOS still have much to achieve because machine learning models can tolerate imprecise computation. In 2015, Suyong Gupta, et al. demonstrated in their ICML paper Deep learning with limited numerical precision that in fact reduced-precision models have equivalent accuracy to today's standard 64 bit, but using as few as 14 bits of floating point precision. We see this reduced precision, faster computation trend contributing to the 2.5X-per-year improvement at least through the year 2022.
    That gives us about five years to get beyond the von Neumann bottleneck, and to analog devices. Moving data to and from memory slows down deep learning network training. So finding analog devices that can combine memory and computation will be important for neuromorphic computing progress.
<br>
<br>
</p>The future of  Software is Neuromorphic Computing</p>
<br>
    Neuromorphic computing, as it sounds, mimics brain cells. Its architecture of interconnected "neurons" replace von-Neumann's back-and-forth bottleneck with low-powered signals that go directly between neurons for more efficient computation. The US Air Force Research Lab is testing a 64-chip array of our IBM TrueNorth Neurosynaptic System designed for deep neural-network inferencing and information discovery. The system uses standard digital CMOS but only consumes 10 watts of energy to power its 64 million neurons and 16 billion synapses.
    But phase change memory, a next-gen memory material, may be the first analog device optimized for deep learning networks. How does a memory – the very bottleneck of von-Neumann architecture – improve machine learning? Because we've figured out how to bring computation to the memory. Recently, IBM scientists demonstrated in-memory computing with 1 million devices for applications in AI, publishing their results, Temporal correlation detection using computational phase-change memory, in Nature Communications, and also presenting it at the IEDM session Compressed Sensing Recovery using Computational Memory.
    Analog computing's maturity will extend the 2.5X-per-year machine learning improvement for a few more years, to 2026 or thereabout.
    While currently utilizing just a few qubits, algorithms run on the free and open IBM Q experience systems are already showing the potential for efficient and effective use in chemistry, optimization, and even machine learning. A paper IBM researchers co-authored with scientists from Raytheon BBN, "Demonstration of quantum advantage in machine learning" in Nature Quantum Information demonstrates how, with only a five superconducting quantum bit processor, the quantum algorithm consistently identified the sequence in up to a 100-fold fewer computational steps and was more tolerant of noise than the classical (non-quantum) algorithm.
    IBM Q's commercial systems now have 20 qubits, and a prototype 50 qubit device is operational. Its average coherence time of 90µs is also double that of previous systems. But a fault tolerant system that shows a distinct quantum advantage over today's machines is still a work in progress. In the meantime, experimenting with new materials (like the replacement of copper interconnects) is key – as are other crucial chip improvements IBM and its partners presented at IEDM in the name of advancing all computing platforms, from von Neumann, to neuromorphic, and quantum.
<br>
<br>
</p>Computer Hardware and Software for the Generation of Virtual Environments</p>
<br>
    The computer technology that allows us to develop three-dimensional virtual environments (VEs) consists of both hardware and software. The current popular, technical, and scientific interest in VEs is inspired, in large part, by the advent and availability of increasingly powerful and affordable visually oriented, interactive, graphical display systems and techniques. One possible organization of the computer technology for VEs is to decompose it into functional blocks.
<br>
<br>
</p>1)Rendering hardware and software for driving modality-specific display devices.</p>
<br>
</p>  2)Hardware and software for modality-specific aspects of models and the generation of corresponding display representations.</p>
<br>
</p>    3)The core hardware and software in which modality-independent aspects of models as well as consistency and registration among multimodal models are taken into consideration.</p>
<br>  
The minimum acceptable rate for VE is lower, reflecting the trade-offs between cost and such tolerances. With regard to computer hardware, there are several senses of frame rate: they are roughly classified as graphical, computational, and data access. Graphical frame rates are critical in order to sustain the illusion of presence or immersion in a VE.</p>
<br>
<br>    
</p>Diego Ristè et al. Demonstration of quantum advantage in machine learning, npj Quantum Information (2017). DOI: 10.1038/s41534-017-0017-3</p>
<br>                          
</p>8 Computer Hardware and Software for the Generation of Virtual Environments." National Research Council. 1995. Virtual Reality: Scientific and Technological Challenges. Washington, DC: The National Academies Press. DOI: 10.17226/4761.</P>
    
Let's have fun writing information in here</p>
<br>
<br>
<p>Yuki Candelario</p>
<p>Mwangi Samuel</p>
<p>Andree Portal</p>
<p>Stefan Buruiana</p>
</section>
<!-- Side BarStart -->

</main>

<footer>
<p>Copyright &copy; Seattle Central College SD& T SP20 Year Up.Org</p>
    <!-- Nav Bar Start -->
<nav>
    <a href="#" title="Class Assignments">About Us</a> &nbsp;|&nbsp;
    <a href="#" title="Class Assignments">Terms And Conditions</a> &nbsp;|&nbsp;
    <a href="#" title="Home Assignments">Dates and sortation</a> &nbsp;|&nbsp;
    <a href="#" title="Resourses">Resources Files and Reducted</a> &nbsp;|&nbsp;
   
    </nav>
    <!-- Nav End-->
</p>
</footer>

</div>
</body>
</html>